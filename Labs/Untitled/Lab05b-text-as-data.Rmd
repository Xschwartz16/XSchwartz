---
title: "Lab 5b - Text as Data"
subtitle: "Emily Dickinson Poems"
date: "`r Sys.Date()`"
always_allow_html: yes
output:
  html_document:
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes 
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE}
knitr::opts_chunk$set(eval = TRUE
                      , warning = FALSE
                      , message = FALSE)
# change displayed number of digits in R Markdown document
options(digits = 2)
```

Next Tuesday (Feb. 25th), we'll be visiting the Emily Dickinson Museum.  Be sure to arrive there by 1:00!  Our tour starts promptly at 1:00.  

Today we're going to analyze poetry by Emily Dickinson.  I scraped most of Emily Dickison's poems from the internet using an iterated procedure.  We'll learn about web scraping later in the semester, but the process of scrapping the poems across different webpages serves as another example of algorithmic thinking.

# Packages

In this lab we will work with the familiar `tidyverse` and `janitor` packages.  There are three new packages we'll be using: (1) the `tidytext` package, which makes text analysis easier and is consistent with the tools we've been using in the `tidyverse` package; (2) the `wordcloud` package which allows us to visually represent the text data in wordclouds; and (3) the `textdata` package which allows us to access lexicons for sentiment analysis.  If working on your own machine, you may need to install these packages before loading (using the `install.packages()` command or by going to Tools > Install Packages) .

```{r message=FALSE}
library(tidyverse) 
library(janitor)

library(tidytext)
library(wordcloud)
library(textdata)
```

# Web scraping the poems

Wikipedia has a webpage with a list of the first lines (often used as titles) for all of Emily Dickinson's poems:

https://en.wikipedia.org/wiki/List_of_Emily_Dickinson_poems

Most of them link to another Wikipedia page that has the poem text.  For example:

https://en.wikisource.org/wiki/A_Bee_his_burnished_Carriage

I used the first page (with the list of poems) to identify the possible URLs I should be visiting to scrape the text of the poems, then scraped them all into an R dataframe, which I then output into a csv file.  

I saved the csv file in a public drive so you can access it using the following code: 

```{r}
poems <- read.csv("http://kcorreia.people.amherst.edu/F1920/DickinsonPoems.csv"
                  , stringsAsFactors = FALSE) %>%
  filter(text != "Still Missing")
```

\newpage
# Tidy text

## tokenizing

The `unnest_tokens` function from the `tidytext` package takes on two main arguments: `output` and `input`.  `output` assigns a variable name for the new variable that will hold the words, and `input` identifies the variable in your dataframe that holds the text.  For instance, in the code below, we're unnesting the variable `text` and the new data frame `poems_words` will contain a variable `word`.

```{r}
poems_words <- poems %>%
  unnest_tokens(output = word, input = text)
```

The default unit for tokenizing is a word.  But you can specify the `token=` option to tokenize the text by other functions, such as "characters", "ngrams", "sentences", or "lines", among other options.  Try one!  See how it changes the output dataset.

```{r}

poems_lines <- poems %>%
  unnest_tokens(output = word, input = text, token = "lines")

poems_words_by_six <- poems %>%
  unnest_tokens(output = word, input = text, token = "ngrams", n=29)

```

## removing stop words

Many commonly used words like "the", "if", and "or" don't provide any insight into the text and are not useful for an analysis.  These are called stop words, and are often removed before analysis.   

The `tidytext` package  provides a dataframe with stop words from three differnt lexicons (onix, snowball, and SMART).  

We can use this `stop_words` dataset to remove all the stop words from our `poems_words` dataset.  

```{r}
# first, take a look at the stop_words dataset
data(stop_words)
head(stop_words)
tail(stop_words)
stop_words %>% count(lexicon)

# goes from n=77,148 rows to n=31,511 words
# 45,637 stop words removed!
poems_words2 <- poems_words %>%
  anti_join(stop_words, by="word")

# check stop words removed
# note that if you didn't want all these words removed, you could modify the stop_words dataframe before anti_joining above ...
removed <- poems_words %>%
  anti_join(poems_words2, by="word") %>%
  count(word) %>%
  arrange(word)

head(removed)
```

We can now use functions we already know and love to create a simple plot of the 10 most common words used, for instance.

```{r}
poems_words2 %>%
  count(word, sort = TRUE) %>% 
  slice(1:10) %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(x = word, y = n, color = word, fill=word)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Number of instances"
       , title="The most common words in Emily Dickinson's poems") +
  guides(color = "none", fill = "none")
```

Note that if we hadn't removed the stop words first, all of the most common words identified would be stop words:

```{r}
poems_words %>%
  count(word, sort = TRUE) %>% 
  slice(1:10) %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(x = word, y = n, color = word, fill=word)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Number of instances"
       , title="The most common words in Emily Dickinson's poems") +
  guides(color = "none", fill = "none")
```


# Term frequency 

To recap, it really only took 4 lines of code to get from our scraped dataset to a dataset formatted for plotting word frequencies:

```{r}
word_frequencies <- poems %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words, by="word") %>%
  count(word, sort = TRUE) 
```

## Word cloud

Word clouds can be used as a quick visualization of the prevalence of words in a corpus.  A bare-bones word cloud using the `wordcloud` function from the `wordcloud` package:

```{r}
# using piping 
word_frequencies %>%
  with(wordcloud(word, n, max.words=50))

# referencing variables directly
wordcloud(word_frequencies$word
          ,word_frequencies$n, max.words=50)
```

Map size *and color* to frequency:

```{r}
# choose color palette from color brewer
pal <- brewer.pal(10, "Paired")

wordcloud(word_frequencies$word,word_frequencies$n
          , min.freq=20
          , max.words=50
          # plot the words in a random order
          , random.order=T
          # specify the range of the size of the words
          , scale=c(2,0.3)
          # specify proportion of words with 90 degree rotation
          , rot.per=.15
          # colors words from least to most frequent
          , colors = pal
          # font family
          , family="sans")
```

Your turn: create a word cloud with 100 words.

```{r}

```


## tf-idf

The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a corpus, in this case, the collection of Emily Dickinson's poems as a whole.

The `bind_tf_idf` function will compute the tf, idf, and tf-idf statistics for us so long as we provide a dataset that is one row per poem-word (that is, one row per word per poem).  We need a variable to indicate which poem the word comes from, a variable to indicate the word, and a third variable to indicate the number of times that word appears in that specific poem.

This time, we do not need to remove stop words.  Why not?

> ANSWER: 

```{r}
word_freqs_by_poem <- poems %>%
  unnest_tokens(output = word, input = text) %>%
  group_by(name, pub_year) %>%
  count(word) 

tfidf <- word_freqs_by_poem %>%
  bind_tf_idf(term = word, document = name, n = n)
```

We can visualize the high tf-idf words using the code below:  

```{r}
set.seed(20191024)
samp <- sample(poems$name, size = 4)

top_tfidf <- tfidf %>%
  filter(name %in% samp) %>%
  arrange(desc(tf_idf)) %>%
  group_by(name) %>%
  slice(1:10) %>% 
  ungroup()

ggplot(data=top_tfidf, aes(x = reorder(word,tf_idf)
                           , y = tf_idf
                           , fill = name)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~ name, ncol = 2, scales = "free") +
  coord_flip()
```


# Sentiment analysis

What is sentiment analysis?  From Text Mining with R (Silge & Robinson 2019):

"When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust. We can use the tools of text mining to approach the emotional content of text programmatically . . . One way to analyze the sentiment of a text is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words. This isn't the only way to approach sentiment analysis, but it is an often-used approach."

There are different lexicons that can be used to classify the sentiment of text.  Today, we'll compare two different lexicons that are both based on unigrams, the AFINN lexicon and the NRC lexicon.

The AFINN lexicon (Nielsen 2011) assigns words a score from -5 (negative sentiment) to +5 (positive sentiment).  Check out the AFINN lexicon using the code below.  What do you think of the scores?  What is the rating for the word "slick"?  Does "slick" always have a positive connotation (can you think of a sentence where "slick" has a negative connotation)?

> ANSWER:

```{r}
afinn_lexicon <- get_sentiments("afinn")

```

Use the `get_sentiments` function to create a dataframe `nrc_lexicon` that holds the NRC Word-Emotion Association lexicon (Mohammad 2010).  The NRC lexicon catergoizes words as yes/no for the following sentiment categories: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.  What does each row in this dataset represent? (Hint: it's *not* the same as the `afinn_lexicon` dataset.)

> ANSWER:

```{r}
nrc_lexicon <- get_sentiments("nrc")

```


User (and Consumer!) Beware: 

- Do you see any issues in applying these lexicons (developed fairly recently) to the Emily Dickinson poems?

> ANSWER:  

- The lexicons are based on unigrams.  Do you see any disadvantages of basing the sentiment on single words?

> ANSWER: 


We can calculate how many words used in the poems are not found in the lexicons.  Run the code below.  List a few words that are not in the NRC lexicon that appear in the poems.  What proportion of unigrams observed within this corpora of Dickinson poems are *not* scored by the NRC lexicon?  

> ANSWER: 

```{r}
# identify words in word_frequencies dataset (which has stop words removed) that are not the NRC lexicon
nrc_missed_words <- word_frequencies %>%
  anti_join(nrc_lexicon, by="word")
```

With these (rather important!) drawbacks in mind, let's go ahead and view the top words by sentiment classified by the NRC lexicon. That is, create a figure of the top 10 words under each sentiment, facetted by sentiment, for the following sentiments: anger, anticipation, fear, joy, surprise, and trust.  You can use code given in earlier chunks to guide you.

```{r}


```


How might one summarize the sentiment of this corpus using the AFINN lexicon?

> ANSWER:



\newpage
# References

### AFINN Lexicon

Nielsen, FA. A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages 718 in CEUR Workshop Proceedings 93-98. 2011 May. http://arxiv.org/abs/1103.2903.

### NRC Lexicon

Mohammad S, Turney P. Crowdsourcing a Word-Emotion Association Lexicon. \textit{Computational Intelligence}. 2013;29(3):436-465.    

Mohammad S, Turney P. Emotions Evoked by Common Words and Phrases: Using Mechanical Turk to Create an Emotion Lexicon. In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, June 2010, LA, California.   

http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm


### Text Mining with R

Silge J, Robinson D (2016). "tidytext: Text Mining and Analysis Using Tidy Data Principles in R." _JOSS_, *1*(3). doi: 10.21105/joss.00037 (URL: https://doi.org/10.21105/joss.00037).

Silge J, Robinson D (2017). Text Mining with R: A Tidy Approach. O'Reilly Media Inc. Sebastopol, CA.  

https://www.tidytextmining.com/index.html
